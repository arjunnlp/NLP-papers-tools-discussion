## NLP-papers-tools-discussion
Anything useful goes here

### Possible research directions and Fundamental discussions

[The 4 Biggest Open Problems in NLP Today](http://ruder.io/4-biggest-open-problems-in-nlp/)

[What innate priors should we build into the architecture of deep learning systems? A debate between Prof. Y. LeCun and Prof. C. Manning](https://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html)

### New Libraries/SOTA we care about

[Flair - LM/Embedding/General NLP lib. Fastest growing NLP project on github](https://github.com/zalandoresearch/flair)

### Visualization Software

General:

[TensorboardX for PyTorch](https://github.com/arjunnlp/tensorboardX)

[Visdom - similar to tensorboard](https://github.com/arjunnlp/visdom)

LSTM:

[LSTMVis: Visualizng LSTM](https://github.com/HendrikStrobelt/LSTMVis)

[Seq2Seq Vis: Visualization for Sequential Neural Networks with Attention](https://github.com/HendrikStrobelt/Seq2Seq-Vis)

### Other useful tools

PyTorch tools:

[ipyexperiments - will save you 20-30% video and 10-15% system memory](https://github.com/stas00/ipyexperiments)

[ipyexperiof usage s in: ](https://github.com/arjunnlp/NLP-papers-tools-discussion/blob/master/preprocess-dainis.ipynb)

Make sure to either use IPyTorchExperiments all the time, or IPyCPUExperiments if don't care to use GPU. If you are using a GPU, you must be sure to use the IPyTorchExperiments and that the text after the cell tells you it is indeed using GPU backend.

### Paper and Technical Writing HOWTO

On writing research papers:

["How to Write an Introduction" by Dr. Om Gnawali](http://www2.cs.uh.edu/~gnawali/courses/cosc6321-s17/hw7.html)

Some of the best examples of technical writing (papers & blogs go hand in hand!):

[How to trick a neural network into thinking a panda is a vulture](https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture)

[Picking an optimizer for Style Transfer](https://blog.slavv.com/picking-an-optimizer-for-style-transfer-86e7b8cba84b)

[How do we 'train' Neural Networks?](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73)

### Must-read papers
[Deep Graph Methods Survey](https://arxiv.org/pdf/1901.00596.pdf)

[Disciplined Training of Neural Networks](https://arxiv.org/abs/1803.09820)

### Blogs You Should Follow

[Anrej Karpathy](https://medium.com/@karpathy)

[Vitaliy Bushaev](https://towardsdatascience.com/@bushaev)

[Sylvain Gugger](https://sgugger.github.io/)

[Sebastian Ruder](http://ruder.io/)

[Ilya Sutskever](https://blog.openai.com/tag/ilya-sutskever/) - Sutskever published his Transformer in this blog, not even on arxiv

[Jeremy Howard](https://twitter.com/jeremyphoward) 
